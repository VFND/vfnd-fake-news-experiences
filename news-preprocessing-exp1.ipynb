{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền xử lý dữ liệu\n",
    "\n",
    "Mục tiêu: chúng ta load được các dữ liệu từ các file JSON, sau đó sẽ loại bỏ:\n",
    "1. Cleaning: Loại bỏ các dấu câu đã được chuyển thành NCR (numeric character reference)\n",
    "2. Tokenizer -> sau đó loại bỏ Stopwords\n",
    "3. \\[Optional\\] Chuyển từ viết tắt thành từ đầy đủ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata as ud\n",
    "import sys\n",
    "\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_json&#95;files_: List chứa các file JSON trong data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_news = \"Data/\"\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_news) if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real_1.json', 'fake_1.json']\n"
     ]
    }
   ],
   "source": [
    "print(json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo dataframe bằng [pandas.DataFrame](http://pandas-docs.github.io/pandas-docs-travis/generated/pandas.Series.loc.html) để có thể lưu các thuộc tính load từ các file JSON đã được load ở trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame(columns=['title', 'text', 'authors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dữ liệu từ các file JSON vào list news theo các thuộc tính: _title_, _text_ và _authors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_news, js), 'r', encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "        \n",
    "        title = json_text['title']\n",
    "        text = json_text['text']\n",
    "        author = json_text['authors']\n",
    "        \n",
    "        news.loc[index] = [title, text, author]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loại bỏ dấu câu\n",
    "\n",
    "Trong dữ liệu sau khi được crawl về, các ký tự đặc biệt (ví dụ các dấu câu) sẽ được đổi thành dạng [numeric character reference (ncr)](https://en.wikipedia.org/wiki/Numeric_character_reference). Ví dụ như: Trong file _real&#95;1.json_ phần title (ở bên dưới). \n",
    "\n",
    "Chúng ta phải đổi lại dưới dạng Unicode để có thể loại bỏ phần dấu câu. \n",
    "\n",
    "_html.unescape(str)_: đổi các kí tự dạng __ncr__ về định dạng Unicode của chúng\n",
    "\n",
    "Tham khảo [stackoverflow](https://stackoverflow.com/a/17018874/5144980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_str = news.loc[0]['title']\n",
    "\n",
    "title_str = html.unescape(title_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_remove&#95;punc_: hàm dùng để loại bỏ các kí tự đặc biệt như dấu câu trong định dạng Unicode. Tham khảo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-8ca530440038>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-8ca530440038>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    def remove_punc(text)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                       if ud.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punc(text)\n",
    "    return text.translate(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ví dụ:__ Title của _real&#95;1.json_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title của _real&#95;1.json_ sau khi loại bỏ kí tự đặc biệt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = remove_punc(title_str)\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Bắt đầu xử lý các dữ liệu trong list các file JSON và lưu trong list ___preproc&#95;news___:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preproc_news = pd.DataFrame(columns=['title', 'text', 'authors'])\n",
    "\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_news, js), 'r', encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "        \n",
    "        title = remove_punc(html.unescape(json_text['title']))\n",
    "        text = remove_punc(html.unescape(json_text['text']))\n",
    "        author = json_text['authors']\n",
    "        \n",
    "        preproc_news.loc[index] = [title, text, author]\n",
    "        \n",
    "print(preproc_news.loc[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer & Loại StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước hết cần biểu diễn token sau đó loại bỏ đi các stopword\n",
    "\n",
    "Dữ liệu từ điển được lấy từ (các file được chứa trong thư mục ./corpus):\n",
    "1. [deepai-solutions/core_nlp](https://github.com/deepai-solutions/core_nlp/tree/master/tokenization): 2 file _bi_\\__gram.txt_ và _tri_\\__gram.txt_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cách 1: Sử dụng so khớp dài nhất \n",
    "\n",
    "Sử dụng so khớp dài nhất (longest matching) để tách các từ trong  tin tức vừa loại dấu câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re #Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_load_\\__n_\\__gram_: hàm đọc dữ liệu trong các corpus và đưa vào  chương trình với kiểu dữ liệu là ___Sets___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_gram(file_path):\n",
    "    with open(file_path, encoding='utf-8') as n_gram:\n",
    "        words = n_gram.read()\n",
    "        words = ast.literal_eval(words)\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đường dẫn đến __corpus__ và tên các file corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = './corpus/'\n",
    "bi_gram_path = 'bi_gram.txt'\n",
    "tri_gram_path = 'tri_gram.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_bi_\\__gram_ và _tri_\\__gram_ là các tập dữ liệu từ các corpus của chúng ta. Ở đoạn kế tiếp chúng ta có thể thấy số lượng phần tử các corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram = load_n_gram(os.path.join(corpus_path, bi_gram_path))\n",
    "tri_gram = load_n_gram(os.path.join(corpus_path, tri_gram_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('2-gram: ', len(bi_gram), '& 3-gram: ', len(tri_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ud.normalize('NFC', preproc_news.loc[0].text)\n",
    "word = \"\\w+\"\n",
    "token = re.findall(word, text, re.UNICODE)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thử nghiệm chính của _LongestMatching_. Tham khảo chủ yếu từ: [ core_nlp/tokenization/dict_models.py](https://github.com/deepai-solutions/core_nlp/blob/master/tokenization/dict_models.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LongestMatching(token, bi_gram, tri_gram):\n",
    "    token_len = len(token)\n",
    "#     print(token_len)\n",
    "    cur_id = 0\n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while (cur_id < token_len) and (not done):\n",
    "        cur_word = token[cur_id]\n",
    "        if(cur_id >= token_len - 1):\n",
    "            word_list.append(cur_word)\n",
    "            done = True\n",
    "#             print(cur_id)\n",
    "        else:\n",
    "            next_word = token[cur_id + 1]\n",
    "            bi_word = \" \".join([cur_word.lower(), next_word.lower()])\n",
    "#             print(bi_word)\n",
    "            if(cur_id >= token_len - 2):\n",
    "                    if bi_word in bi_gram:\n",
    "                        word_list.append(\"_\".join([cur_word, next_word]))\n",
    "                        cur_id  = cur_id + 2\n",
    "                    else: \n",
    "                        word_list.append(cur_word)\n",
    "                        cur_id  = cur_id + 1\n",
    "                        \n",
    "            else: \n",
    "                next_next_word = token[cur_id + 2]\n",
    "                tri_word = \" \".join([bi_word, next_next_word.lower()])\n",
    "                if tri_word in tri_gram:\n",
    "                    word_list.append(\"_\".join([cur_word, next_word, next_next_word]))\n",
    "                    cur_id  = cur_id + 3\n",
    "#                     print(tri_word)\n",
    "                elif bi_word in bi_gram:\n",
    "                    word_list.append(\"_\".join([cur_word, next_word]))\n",
    "                    cur_id = cur_id + 2\n",
    "#                     print(bi_word)\n",
    "                else:\n",
    "                    word_list.append(cur_word)\n",
    "                    cur_id = cur_id + 1\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_text_\\__tokenize_ là bộ dữ liệu sau khi đã tách các từ thành các token và có thể đưa vào mô hình train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenize = LongestMatching(token, bi_gram, tri_gram)\n",
    "text_for_train = \" \".join(text_tokenize)\n",
    "print(text_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
