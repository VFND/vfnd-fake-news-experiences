{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền xử lý dữ liệu\n",
    "\n",
    "Mục tiêu: chúng ta load được các dữ liệu từ các file JSON, sau đó sẽ loại bỏ:\n",
    "1. Cleaning: Loại bỏ các dấu câu đã được chuyển thành NCR (numeric character reference)\n",
    "2. Tokenizer -> sau đó loại bỏ Stopwords\n",
    "3. \\[Optional\\] Chuyển từ viết tắt thành từ đầy đủ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata as ud\n",
    "import sys\n",
    "\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_json&#95;files_: List chứa các file JSON trong data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_news = \"Data/\"\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_news) if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real_1.json', 'fake_1.json']\n"
     ]
    }
   ],
   "source": [
    "print(json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo dataframe bằng [pandas.DataFrame](http://pandas-docs.github.io/pandas-docs-travis/generated/pandas.Series.loc.html) để có thể lưu các thuộc tính load từ các file JSON đã được load ở trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame(columns=['title', 'text', 'authors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dữ liệu từ các file JSON vào list news theo các thuộc tính: _title_, _text_ và _authors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_news, js), 'r', encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "        \n",
    "        title = json_text['title']\n",
    "        text = json_text['text']\n",
    "        author = json_text['authors']\n",
    "        \n",
    "        news.loc[index] = [title, text, author]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loại bỏ dấu câu\n",
    "\n",
    "Trong dữ liệu sau khi được crawl về, các ký tự đặc biệt (ví dụ các dấu câu) sẽ được đổi thành dạng [numeric character reference (ncr)](https://en.wikipedia.org/wiki/Numeric_character_reference). Ví dụ như: Trong file _real&#95;1.json_ phần title (ở bên dưới). \n",
    "\n",
    "Chúng ta phải đổi lại dưới dạng Unicode để có thể loại bỏ phần dấu câu. \n",
    "\n",
    "_html.unescape(str)_: đổi các kí tự dạng __ncr__ về định dạng Unicode của chúng\n",
    "\n",
    "Tham khảo [stackoverflow](https://stackoverflow.com/a/17018874/5144980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_str = news.loc[0]['title']\n",
    "\n",
    "title_str = html.unescape(title_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_remove&#95;punc_: hàm dùng để loại bỏ các kí tự đặc biệt như dấu câu trong định dạng Unicode. Tham khảo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                       if ud.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ví dụ:__ Title của _real&#95;1.json_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tung tin 'Mr Bean' qua đời để phát tán virus độc hại\n"
     ]
    }
   ],
   "source": [
    "print(title_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title của _real&#95;1.json_ sau khi loại bỏ kí tự đặc biệt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tung tin Mr Bean qua đời để phát tán virus độc hại\n"
     ]
    }
   ],
   "source": [
    "new_string = remove_punc(title_str)\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Bắt đầu xử lý các dữ liệu trong list các file JSON và lưu trong list ___preproc&#95;news___:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hôm 197 nhiều độc giả cảnh báo nhau trên mạng về một loại virus máy tính phát tán qua tin giả Mr Bean  Rowan Atkinson qua đời mạo danh Fox News\n",
      "\n",
      "Thông tin lan truyền cách đây ít phút trên mạng xã hội Theo trang SDE các đối tượng phát tán những bài đăng giả trên mạng xã hội với hình thức như một đường link tin tức tiêu đề là Tin nóng của Fox News Mr Bean Rowan Atkinson vừa qua đời ở tuổi 62 sau một vụ đâm xe trong lúc đóng cảnh hành động mạo hiểm Hôm nay tháng 72017\n",
      "Tung tin Mr Bean qua doi de phat tan virus doc hai hinh anh 1\n",
      "Tin cải chính về cái chết của Rowan Atkinson được đưa ra kịp thời để bảo vệ người dùng khỏi virus Ảnh Chronicle Live \n",
      "\n",
      "Vì gắn với một cơ quan truyền thông lớn là Fox News nên mẩu tin dễ dàng thu hút sự chú ý và tin cậy của độc giả dù thông tin ngày tháng bị sai lệch Trang SDE cảnh báo bất cứ độc giả nào nhìn thấy đường link chứa thông tin này không được bấm vào vì thực chất đó là một virus đội lốt\n",
      "\n",
      "Đường link sẽ dẫn thẳng đến một trang web lừa đảo và mã độc nhảy vào máy tính của người truy cập\n",
      "\n",
      "Đây không phải lần đầu tiên Rowan Atkinson trở thành nạn nhân của tin đồn qua đời Trước đây vào năm 2016 và tháng 3 năm nay từng có một loạt bài thất thiệt khẳng định tài tử Johnny English đã tử vong trong một tai nạn xe ở Los Angeles\n",
      "\n",
      "Thông tin về các ngôi sao qua đời cũng thường xuyên được làm giả để lan truyền virus độc hại Trang web Hoax Slayer đã thống kê hầu hết trường hợp tương tự để cảnh báo độc giả\n",
      "Tung tin Mr Bean qua doi de phat tan virus doc hai hinh anh 2\n",
      "Rất nhiều người nổi tiếng bị kẻ gian tung tin qua đời vì mục đích xấu Ảnh CNN\n",
      "\n",
      "Những tin giả này thường dẫn người dùng đến các trang web cảnh báo máy tính đã bị xâm nhập và yêu cầu người dùng gọi đến số điện thoại hỗ trợ Nhưng khi họ gọi đến những kẻ lừa đảo sẽ thu thập thông tin tài khoản ngân hàng của họ với mục đích thanh toán tiền sửa chữa máy tính hoặc tải phần mềm bảo vệ máy\n",
      "\n",
      "Thực chất đó là các phần mềm kiểm soát máy tính và ăn cắp dữ liệu cài mã độc từ xa\n"
     ]
    }
   ],
   "source": [
    "preproc_news = pd.DataFrame(columns=['title', 'text', 'authors'])\n",
    "\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_news, js), 'r', encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "        \n",
    "        title = remove_punc(html.unescape(json_text['title']))\n",
    "        text = remove_punc(html.unescape(json_text['text']))\n",
    "        author = json_text['authors']\n",
    "        \n",
    "        preproc_news.loc[index] = [title, text, author]\n",
    "        \n",
    "print(preproc_news.loc[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer & Loại StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước hết cần biểu diễn token sau đó loại bỏ đi các stopword\n",
    "\n",
    "Dữ liệu từ điển được lấy từ (các file được chứa trong thư mục ./corpus):\n",
    "1. [deepai-solutions/core_nlp](https://github.com/deepai-solutions/core_nlp/tree/master/tokenization): 2 file _bi_\\__gram.txt_ và _tri_\\__gram.txt_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cách 1: Sử dụng so khớp dài nhất \n",
    "\n",
    "Sử dụng so khớp dài nhất (longest matching) để tách các từ trong  tin tức vừa loại dấu câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re #Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_load_\\__n_\\__gram_: hàm đọc dữ liệu trong các corpus và đưa vào  chương trình với kiểu dữ liệu là ___Sets___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_gram(file_path):\n",
    "    with open(file_path, encoding='utf-8') as n_gram:\n",
    "        words = n_gram.read()\n",
    "        words = ast.literal_eval(words)\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đường dẫn đến __corpus__ và tên các file corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = './corpus/'\n",
    "bi_gram_path = 'bi_gram.txt'\n",
    "tri_gram_path = 'tri_gram.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_bi_\\__gram_ và _tri_\\__gram_ là các tập dữ liệu từ các corpus của chúng ta. Ở đoạn kế tiếp chúng ta có thể thấy số lượng phần tử các corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram = load_n_gram(os.path.join(corpus_path, bi_gram_path))\n",
    "tri_gram = load_n_gram(os.path.join(corpus_path, tri_gram_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram:  22705 & 3-gram:  1907\n"
     ]
    }
   ],
   "source": [
    "print ('2-gram: ', len(bi_gram), '& 3-gram: ', len(tri_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hôm', '197', 'nhiều', 'độc', 'giả', 'cảnh', 'báo', 'nhau', 'trên', 'mạng', 'về', 'một', 'loại', 'virus', 'máy', 'tính', 'phát', 'tán', 'qua', 'tin', 'giả', 'Mr', 'Bean', 'Rowan', 'Atkinson', 'qua', 'đời', 'mạo', 'danh', 'Fox', 'News', 'Thông', 'tin', 'lan', 'truyền', 'cách', 'đây', 'ít', 'phút', 'trên', 'mạng', 'xã', 'hội', 'Theo', 'trang', 'SDE', 'các', 'đối', 'tượng', 'phát', 'tán', 'những', 'bài', 'đăng', 'giả', 'trên', 'mạng', 'xã', 'hội', 'với', 'hình', 'thức', 'như', 'một', 'đường', 'link', 'tin', 'tức', 'tiêu', 'đề', 'là', 'Tin', 'nóng', 'của', 'Fox', 'News', 'Mr', 'Bean', 'Rowan', 'Atkinson', 'vừa', 'qua', 'đời', 'ở', 'tuổi', '62', 'sau', 'một', 'vụ', 'đâm', 'xe', 'trong', 'lúc', 'đóng', 'cảnh', 'hành', 'động', 'mạo', 'hiểm', 'Hôm', 'nay', 'tháng', '72017', 'Tung', 'tin', 'Mr', 'Bean', 'qua', 'doi', 'de', 'phat', 'tan', 'virus', 'doc', 'hai', 'hinh', 'anh', '1', 'Tin', 'cải', 'chính', 'về', 'cái', 'chết', 'của', 'Rowan', 'Atkinson', 'được', 'đưa', 'ra', 'kịp', 'thời', 'để', 'bảo', 'vệ', 'người', 'dùng', 'khỏi', 'virus', 'Ảnh', 'Chronicle', 'Live', 'Vì', 'gắn', 'với', 'một', 'cơ', 'quan', 'truyền', 'thông', 'lớn', 'là', 'Fox', 'News', 'nên', 'mẩu', 'tin', 'dễ', 'dàng', 'thu', 'hút', 'sự', 'chú', 'ý', 'và', 'tin', 'cậy', 'của', 'độc', 'giả', 'dù', 'thông', 'tin', 'ngày', 'tháng', 'bị', 'sai', 'lệch', 'Trang', 'SDE', 'cảnh', 'báo', 'bất', 'cứ', 'độc', 'giả', 'nào', 'nhìn', 'thấy', 'đường', 'link', 'chứa', 'thông', 'tin', 'này', 'không', 'được', 'bấm', 'vào', 'vì', 'thực', 'chất', 'đó', 'là', 'một', 'virus', 'đội', 'lốt', 'Đường', 'link', 'sẽ', 'dẫn', 'thẳng', 'đến', 'một', 'trang', 'web', 'lừa', 'đảo', 'và', 'mã', 'độc', 'nhảy', 'vào', 'máy', 'tính', 'của', 'người', 'truy', 'cập', 'Đây', 'không', 'phải', 'lần', 'đầu', 'tiên', 'Rowan', 'Atkinson', 'trở', 'thành', 'nạn', 'nhân', 'của', 'tin', 'đồn', 'qua', 'đời', 'Trước', 'đây', 'vào', 'năm', '2016', 'và', 'tháng', '3', 'năm', 'nay', 'từng', 'có', 'một', 'loạt', 'bài', 'thất', 'thiệt', 'khẳng', 'định', 'tài', 'tử', 'Johnny', 'English', 'đã', 'tử', 'vong', 'trong', 'một', 'tai', 'nạn', 'xe', 'ở', 'Los', 'Angeles', 'Thông', 'tin', 'về', 'các', 'ngôi', 'sao', 'qua', 'đời', 'cũng', 'thường', 'xuyên', 'được', 'làm', 'giả', 'để', 'lan', 'truyền', 'virus', 'độc', 'hại', 'Trang', 'web', 'Hoax', 'Slayer', 'đã', 'thống', 'kê', 'hầu', 'hết', 'trường', 'hợp', 'tương', 'tự', 'để', 'cảnh', 'báo', 'độc', 'giả', 'Tung', 'tin', 'Mr', 'Bean', 'qua', 'doi', 'de', 'phat', 'tan', 'virus', 'doc', 'hai', 'hinh', 'anh', '2', 'Rất', 'nhiều', 'người', 'nổi', 'tiếng', 'bị', 'kẻ', 'gian', 'tung', 'tin', 'qua', 'đời', 'vì', 'mục', 'đích', 'xấu', 'Ảnh', 'CNN', 'Những', 'tin', 'giả', 'này', 'thường', 'dẫn', 'người', 'dùng', 'đến', 'các', 'trang', 'web', 'cảnh', 'báo', 'máy', 'tính', 'đã', 'bị', 'xâm', 'nhập', 'và', 'yêu', 'cầu', 'người', 'dùng', 'gọi', 'đến', 'số', 'điện', 'thoại', 'hỗ', 'trợ', 'Nhưng', 'khi', 'họ', 'gọi', 'đến', 'những', 'kẻ', 'lừa', 'đảo', 'sẽ', 'thu', 'thập', 'thông', 'tin', 'tài', 'khoản', 'ngân', 'hàng', 'của', 'họ', 'với', 'mục', 'đích', 'thanh', 'toán', 'tiền', 'sửa', 'chữa', 'máy', 'tính', 'hoặc', 'tải', 'phần', 'mềm', 'bảo', 'vệ', 'máy', 'Thực', 'chất', 'đó', 'là', 'các', 'phần', 'mềm', 'kiểm', 'soát', 'máy', 'tính', 'và', 'ăn', 'cắp', 'dữ', 'liệu', 'cài', 'mã', 'độc', 'từ', 'xa']\n"
     ]
    }
   ],
   "source": [
    "text = ud.normalize('NFC', preproc_news.loc[0].text)\n",
    "word = \"\\w+\"\n",
    "token = re.findall(word, text, re.UNICODE)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thử nghiệm chính của _LongestMatching_. Tham khảo chủ yếu từ: [ core_nlp/tokenization/dict_models.py](https://github.com/deepai-solutions/core_nlp/blob/master/tokenization/dict_models.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LongestMatching(token, bi_gram, tri_gram):\n",
    "    token_len = len(token)\n",
    "#     print(token_len)\n",
    "    cur_id = 0\n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while (cur_id < token_len) and (not done):\n",
    "        cur_word = token[cur_id]\n",
    "        if(cur_id >= token_len - 1):\n",
    "            word_list.append(cur_word)\n",
    "            done = True\n",
    "#             print(cur_id)\n",
    "        else:\n",
    "            next_word = token[cur_id + 1]\n",
    "            bi_word = \" \".join([cur_word.lower(), next_word.lower()])\n",
    "#             print(bi_word)\n",
    "            if(cur_id >= token_len - 2):\n",
    "                    if bi_word in bi_gram:\n",
    "                        word_list.append(\"_\".join([cur_word, next_word]))\n",
    "                        cur_id  = cur_id + 2\n",
    "                    else: \n",
    "                        word_list.append(cur_word)\n",
    "                        cur_id  = cur_id + 1\n",
    "                        \n",
    "            else: \n",
    "                next_next_word = token[cur_id + 2]\n",
    "                tri_word = \" \".join([bi_word, next_next_word.lower()])\n",
    "                if tri_word in tri_gram:\n",
    "                    word_list.append(\"_\".join([cur_word, next_word, next_next_word]))\n",
    "                    cur_id  = cur_id + 3\n",
    "#                     print(tri_word)\n",
    "                elif bi_word in bi_gram:\n",
    "                    word_list.append(\"_\".join([cur_word, next_word]))\n",
    "                    cur_id = cur_id + 2\n",
    "#                     print(bi_word)\n",
    "                else:\n",
    "                    word_list.append(cur_word)\n",
    "                    cur_id = cur_id + 1\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_text_\\__tokenize_ là bộ dữ liệu sau khi đã tách các từ thành các token và có thể đưa vào mô hình train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hôm 197 nhiều độc_giả cảnh_báo nhau trên mạng về một loại virus máy_tính phát_tán qua tin giả Mr Bean Rowan Atkinson qua_đời mạo_danh Fox News Thông_tin lan_truyền cách đây ít phút trên mạng xã_hội Theo trang SDE các đối_tượng phát_tán những bài đăng giả trên mạng xã_hội với hình_thức như một đường link tin_tức tiêu_đề là Tin nóng của Fox News Mr Bean Rowan Atkinson vừa_qua đời ở tuổi 62 sau một vụ đâm xe trong lúc đóng cảnh hành_động mạo_hiểm Hôm_nay tháng 72017 Tung tin Mr Bean qua doi de phat tan virus doc hai hinh anh 1 Tin cải_chính về cái chết của Rowan Atkinson được đưa ra kịp_thời để bảo_vệ người dùng khỏi virus Ảnh Chronicle Live Vì gắn với một cơ_quan truyền_thông lớn là Fox News nên mẩu tin dễ_dàng thu_hút sự chú_ý và tin_cậy của_độc giả dù thông_tin ngày_tháng bị sai_lệch Trang SDE cảnh_báo bất_cứ độc_giả nào nhìn thấy đường link chứa thông_tin này không được bấm vào vì thực_chất đó là một virus đội_lốt Đường link sẽ dẫn thẳng đến một trang_web lừa_đảo và mã độc nhảy vào máy_tính của người truy_cập Đây không phải lần đầu_tiên Rowan Atkinson trở_thành nạn_nhân của tin_đồn qua_đời Trước đây vào năm 2016 và tháng 3 năm nay từng có một loạt bài thất_thiệt khẳng_định tài_tử Johnny English đã tử_vong trong một tai_nạn xe ở Los Angeles Thông_tin về các ngôi_sao qua_đời cũng thường_xuyên được làm giả để lan_truyền virus độc_hại Trang_web Hoax Slayer đã thống_kê hầu_hết trường_hợp tương_tự để cảnh_báo độc_giả Tung tin Mr Bean qua doi de phat tan virus doc hai hinh anh 2 Rất nhiều người nổi_tiếng bị kẻ_gian tung tin qua_đời vì mục_đích xấu Ảnh CNN Những tin giả này thường dẫn người dùng đến các trang_web cảnh_báo máy_tính đã bị xâm_nhập và yêu_cầu người dùng gọi đến số điện_thoại hỗ_trợ Nhưng khi họ gọi đến những kẻ lừa_đảo sẽ thu_thập thông_tin tài_khoản ngân_hàng của họ với mục_đích thanh_toán tiền sửa_chữa máy_tính hoặc tải phần_mềm bảo_vệ máy Thực_chất đó là các phần_mềm kiểm_soát máy_tính và ăn_cắp dữ_liệu cài mã độc từ xa\n"
     ]
    }
   ],
   "source": [
    "text_tokenize = LongestMatching(token, bi_gram, tri_gram)\n",
    "text_for_train = \" \".join(text_tokenize)\n",
    "print(text_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
